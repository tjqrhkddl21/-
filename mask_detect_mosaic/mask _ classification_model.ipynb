{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- argparse 모듈 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", default=r'mask_data_preprocessed', help=\"input dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"mask_detector_160x160.model\", help=\"output model\")\n",
    "args = vars(ap.parse_args(args=[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습률, 학습횟수, 배치사이즈를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-4\n",
    "epochs = 3\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- imagepaths에 데이터셋의 모든 이미지를 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagepaths = list(paths.list_images(args[\"dataset\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 경로에서 폴더 라벨로 자름\n",
    "    - label = imagepath.split(os.path.sep)[-2]\n",
    "- 이미지의 크기를 160으로 로드함.\n",
    "    - image = load_img(imagepath, target_size = (160,160))\n",
    "- 이미지를 array로 바꿈\n",
    "    - image = img_to_array(image)\n",
    "- 이미지 전처리 - *\n",
    "    - image = preprocess_input(image)\n",
    "- data에 image 를 저장하고 labels에 label 저장\n",
    "    - data.append(image)\n",
    "    - labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\PIL\\Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for imagepath in imagepaths:\n",
    "    label = imagepath.split(os.path.sep)[-2]\n",
    "    image = load_img(imagepath, target_size = (160,160))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data를 numpy 배열로 변경\n",
    "\n",
    "- 참고 : https://notebook.community/junhwanjang/DataSchool/Lecture/13.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EC%A0%84%EC%B2%98%EB%A6%AC/1)%20Scikit-Learn%EC%9D%98%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Binarizer는 threshold 값을 기준으로 결과를 0,1로 구분한다. default는 0이다.\n",
    "# LabelBinarizer는 원-핫 인코더와 유사하지만, 실제  사용된 클래스만 인코딩에 사용하며,\n",
    "# 0/1 대신 사용할 값을 지정할 수 있다.\n",
    "lb = LabelBinarizer()\n",
    "# lb.fit_transform을 통해 labels에 저장된 폴더에 따라 라벨 값이 0 / 1 로 구분된다.\n",
    "One_Hot_labels = lb.fit_transform(labels)\n",
    "# to_categorical 을 통해 원 핫 벡터 형식으로 바꾼다. ( [0,1]  , [1,0])\n",
    "One_Hot_labels = to_categorical(One_Hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 세트와 test 세트에 8 : 2로 이미지 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x,test_x,train_y,test_y) = train_test_split(data, One_Hot_labels, test_size=0.20, stratify = labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 증강 방법으로 해당 데이터의 전처리를 어떻게 할지 정함.\n",
    "    - width_shift와 height_shift를 통해 좌우, 상하로 이미지를 늘림.\n",
    "    - horizontal 을 통해 수평 반전.\n",
    "    - rotation 을 통해 랜덤하게 각도 내의 범위만큼 돌림.\n",
    "    - zoom 을 통해 랜덤하게 확대.\n",
    "    - shear 를 통해 이미지를 변형시킴. 시계반대방향으로, 강도는 라디안.\n",
    "    - fill_mode는 이미지 회전,이동 시 생기는 공간을 채우는 방식\n",
    "- 참고 : https://srgai.tistory.com/16\n",
    "- https://tykimos.github.io/2017/06/10/CNN_Data_Augmentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rotation_range = 20,\n",
    "                            zoom_range = 0.15,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.15,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode = \"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전이학습을 위한 FC LAYER를 제외한 MovileNetV2 생성\n",
    "- 기존 오픈소스에서 연산 및 예측 부분에서 얼굴에 사용하기엔 224의 이미지가 크고 비효율적인 것 같아 160으로 이미지 크기를 줄이고, Pool_size 또한 5로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = MobileNetV2(weights=\"imagenet\",\n",
    "                        include_top=False,\n",
    "                        input_shape=(160,160,3), \n",
    "                        input_tensor=Input(shape=(160,160,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FC LAYER 추가.\n",
    "\n",
    "- baseModel의 아웃풋을 TopModel로 받음\n",
    "    - TopModel = baseModel.output\n",
    "- 입력 이미지를 pool_size로 나누고 나눈 풀에서 평균값을 뽑아내 출력을 만듬.\n",
    "    - TopModel = AveragePooling2D(pool_size=(5,5))(TopModel)\n",
    "- 입력 이미지를 일차원으로 바꿔줌\n",
    "    - TopModel = Flatten(name='flatten')(TopModel)\n",
    "- flatten으로 일차원으로 바뀐 입력을 받고 출력뉴런을 128개로, 활성화 함수는 은닉층에서 주로 쓰는 relu를 사용\n",
    "    - TopModel = Dense(128, activation='relu')(TopModel)\n",
    "- dropout을 통해 뉴런의 절반을 생략, 줄어든 신경망을 통해 학습 수행할 수 있게함.\n",
    "    - TopModel = Dropout(0.5)(TopModel)\n",
    "- 마지막 LAYER는 분류결과를 계산하는 층으로 mask, no-mask를 위해 출력뉴련은 2개 이고, 활성화 함수는 카테고리 분류에 주로 쓰이는 softmax 사용.\n",
    "    - TopModel = Dense(2, activation = 'softmax')(TopModel)\n",
    "    \n",
    "\n",
    "- 참고 : https://aidalab.tistory.com/22 , \n",
    "- https://m.blog.naver.com/PostView.nhn?blogId=laonple&logNo=220542170499&proxyReferer=https:%2F%2Fwww.google.com%2F\n",
    "- https://tykimos.github.io/2017/01/27/CNN_Layer_Talk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopModel = baseModel.output\n",
    "TopModel = AveragePooling2D(pool_size=(5,5))(TopModel)\n",
    "TopModel = Flatten(name='flatten')(TopModel)\n",
    "TopModel = Dense(128, activation='relu')(TopModel)\n",
    "TopModel = Dropout(0.5)(TopModel)\n",
    "TopModel = Dense(2, activation = 'softmax')(TopModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base 모델 위에 FC LAYER 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs =baseModel.input, outputs = TopModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base 모델은 학습하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss 및 optimizer , metric 설정\n",
    "- decay는 업데이트마다 적용되는 학습률의 감소율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=INIT_LR, decay=INIT_LR / epochs),\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습시킴\n",
    "- 증강방법을 통해 train셋을 배치사이즈로 에포크만큼 학습시킴.\n",
    "- steps_per_epoch은 1epoch 당 몇번의 step을 진행할 것인지, 배치를 몇 번 학습시킬 것인지를 나타내며 주로 총 데이터셋/배치사이즈로 나타냄.\n",
    "- validation_data 는 테스트 셋으로 진행.\n",
    "- validation_step은 테스트셋/ 배치사이즈로 나타냈음.\n",
    "- 모델 학습 과정 epochs=10, batch_size = 512 ==> epochs=20, batch_size=256 ==> epochs=3, batch_size = 128\n",
    "- 모델 학습 결과 42/42 [==============================] - 54s 1s/step - loss: 0.0727 - accuracy: 0.9748 - val_loss: 0.0809 - val_accuracy: 0.9712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "42/42 [==============================] - 54s 1s/step - loss: 0.0788 - accuracy: 0.9733 - val_loss: 0.0810 - val_accuracy: 0.9734\n",
      "Epoch 2/3\n",
      "42/42 [==============================] - 54s 1s/step - loss: 0.0794 - accuracy: 0.9688 - val_loss: 0.0802 - val_accuracy: 0.9734\n",
      "Epoch 3/3\n",
      "42/42 [==============================] - 54s 1s/step - loss: 0.0727 - accuracy: 0.9748 - val_loss: 0.0809 - val_accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "Hist = model.fit(datagen.flow(train_x, train_y, batch_size=batch_size),\n",
    "                 steps_per_epoch = len(train_x) // batch_size,\n",
    "                 validation_data = (test_x, test_y),\n",
    "                 validation_steps= len(test_x) // batch_size,\n",
    "                 epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스트 예측해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24081105, 0.75918895],\n",
       "       [0.9974644 , 0.00253556],\n",
       "       [0.01401506, 0.9859849 ],\n",
       "       ...,\n",
       "       [0.00176875, 0.99823123],\n",
       "       [0.9971391 , 0.00286097],\n",
       "       [0.00122431, 0.99877566]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predldxs = model.predict(test_x, batch_size = batch_size)\n",
    "predldxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률이 제일 높은 예측 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predldxs = np.argmax(predldxs, axis =1)\n",
    "predldxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(args['model'], save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
